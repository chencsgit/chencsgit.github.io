<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Zhengyu Chen's Homepage</title>
<link rel="stylesheet" type="text/css" href="xrb.css">
</head>

<body>
<table width="95%">
<tr>
<th align="left"><h1>Zhengyu Chen (<FONT style="FONT-FAMILY: 楷体">陈政聿</FONT>) </h1></th> 
</tr>
</table>

<table>
<td> <img style="float:left;margin-right:50px" src="images/Ruobing_Xie.jpg" width="210" margin=> </td>
<td width="5"> &nbsp; </td>

<td>

<h4>
Senior Researcher<br>
<a href="https://www.meituan.com/en-US/about-us" target="_blank">Meituan</a><br> 
</h4>

<h4>
Graduated from <a href="http://nlp.csai.tsinghua.edu.cn" target="_blank">Natural Language Processing Lab</a>,<br>
<a href="http://www.cs.tsinghua.edu.cn" target="_blank">Department of Computer Science and Technology</a>,<br>
<a href="http://www.tsinghua.edu.cn" target="_blank">Tsinghua University</a>, Beijing, China.<br> 
</h4>

<h4>
Email: chencsmat [at] 163.com <br>
Google Scholar: <A HREF="https://scholar.google.com/citations?user=PqrvpbkAAAAJ&hl=zh-CN" target=_blank>Zhengyu Chen</A> <br>
Research Interests: Large Language Model, Reinforcement Learning, Natural Language Processing, Information Retrieval <br>
</h4>

<h4>
I am always looking for highly-motivated students to work together in LLM pre-training and post-training.
</h4>

</td>
</table>

<hr>
<h3> <a href="#Bio">Bio</a> | <a href="#Publications">Publications</a> | <a href="#Education">Education and Experience</a> | <a href="#Awards">Awards</a> | <a href="#Academic">Academic Services</a> </h3>


<hr>
<h2><a id=Bio>Bio</a></h2>

Ruobing Xie is a senior researcher of Tencent. He received his BEng degree in 2014 and his master degree in 2017 from the Department of Computer Science and Technology, Tsinghua University. His research interests include Large Language Model, Recommender System, Natural Language Processing, and Information Retrieval. He has published 100+ papers in top-tier conferences and journals including ACL, ICML, NeurIPS, KDD, AAAI, with 10000+ citations according to Google Scholar. He has received the first prize of the Natural Science Award (Science and Technology) of the Ministry of Education, RecSys 2024 Best Full Paper, RecSys 2023 Best Short Paper, the Paper Award Nominations of TheWebConf 2023 (spotlight), EMNLP 2025 SAC highlight, and CCIR 2024 best paper candidate. He is supported by Scholar of Young Talent Promoting Project of CAST, and is the World's Top 2 % Scientists. He is a member of Chinese Information Processing Society of China, Youth Working Committee & Social Media Processing Committee & Information Retrieval Committee. He serves as AC/SPC of ACL, EMNLP, NAACL, AAAI, IJCAI, etc. 

<br><br>
<FONT style="FONT-FAMILY: 楷体">
谢若冰，腾讯高级研究员，2017年毕业于清华大学自然语言处理与社会人文计算实验室。主要研究方向为大语言模型、推荐系统、自然语言处理、信息检索等。他于</FONT>ACL、ICML、NeurIPS、KDD、AAAI<FONT style="FONT-FAMILY: 楷体">等顶会期刊共发表相关论文100余篇，</FONT>Google Scholar<FONT style="FONT-FAMILY: 楷体">统计引用超过10000次，</FONT>h index<FONT style="FONT-FAMILY: 楷体">为47。他曾获教育部自然科学一等奖，世界互联网领先科技成果奖，</FONT>RecSys 2024<FONT style="FONT-FAMILY: 楷体">最佳论文，</FONT>RecSys 2023<FONT style="FONT-FAMILY: 楷体">最佳短文，</FONT>WWW 2023<FONT style="FONT-FAMILY: 楷体">亮点论文（最佳论文提名），</FONT>EMNLP 2025 SAC highlight<FONT style="FONT-FAMILY: 楷体">，</FONT>CCIR 2024<FONT style="FONT-FAMILY: 楷体">最佳论文提名等。他入选了中国科协青年人才托举工程，年度全球前2%顶尖科学家，西贝尔学者，担任中国中文信息学会青年工作委员会委员、社会媒体处理专委会委员、信息检索专委会通讯委员、</FONT>ACL/EMNLP/NAACL/AAAI/IJCAI<FONT style="FONT-FAMILY: 楷体">领域主席/高级程序委员会委员等。</FONT>


<h2><a id=News>News</a></h2>
<ul>
<li><span style="color:red">[Nov. 2025]		Received EMNLP 2025 SAC highlight award!</span>
<li>[Nov. 2025]		Got a paper accepted at AAAI about Transformer/Mamba hybrid model.
<li>[Sep. 2025]		Got a paper accepted at NeurIPS about LLM realignment.
<li>[Aug. 2025]		Got three papers accepted at EMNLP about heterogeneous MoE, Mamba sparsification, and security threat of MLLM.
<li>[Jul. 2025]		Got five papers accepted at ICCV, ACMMM, TKDE about MLLM hallucination detection, MLLM for search, text-to-video retrieval, recommendation fairness.
<li>[Jun. 2025]		Attended at YSSNLP in Dalian.
<li>[May 2025]     We release Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model [<A HREF="https://arxiv.org/abs/2505.15431" target=_blank>technical report</A>].
<li>[May 2025]		Got three papers accepted at ACL about LLM forgetting, LLM model fusion, and MLLM hallucination mitigation.
<li>[May 2025]		Got two papers accepted at ICML about a new routing strategy of MoE and the scaling law of floating point quantization.
<li>[Apr. 2025]		Attended at ICLR in Singapore.
<li>[Apr. 2025]		Got a paper accepted at SIGIR about long-context LLM4Rec.
<li>[Mar. 2025]		Got a paper accepted at CVPR about MLLM hallucination dataset.
<li>[Jan. 2025]     Got six papers accepted at ICLR, NAACL, and WWW about Multi-agent LLM, Multimodal LLM, LLM4Rec, etc.
<li>[Dec. 2024]	    Got five papers accepted at TOIS, AAAI, and WSDM about LLM4Rec, visual representation learning, and cross-domain recommendation.
<li><span style="color:red">[Nov. 2024]     We release Hunyuan-Large, which is currently the best and largest open-source Transformer-based MoE model</span> [<A HREF="https://arxiv.org/abs/2411.02265" target=_blank>technical report</A>] [<A HREF="https://github.com/Tencent/Tencent-Hunyuan-Large" target=_blank>codes</A>] [<A HREF="https://huggingface.co/tencent/Tencent-Hunyuan-Large" target=_blank>models</A>].
<li>[Oct. 2024]		Received the Best Paper Candidate of CCIR 2024!
<li><span style="color:red">[Oct. 2024]		Received the Best Full Paper Award of RecSys 2024!</span>
</ul>

<h2><a id=Publications>Publications</a></h2>
<h3>Journal and Conference Papers</h3>
<h4>(* indicates equal contribution, &dagger; indicates corresponding author.)</h4>
<ol>
<h3><strong>2025</strong></h3>

<li>
	Attributive reasoning for hallucination diagnosis of large language models. <i>Proceedings of the AAAI Conference on Artificial Intelligence 39 (22), 23660 …</i>.<br>
	Y Chen, Z Li, S You, Z Chen, J Chang, Y Zhang, W Dai, Q Guo, Y Xiao.<br>
</li>
<li>
	Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?. <i>arXiv preprint arXiv:2510.11184</i>.<br>
	Z Chen, J Yang, T Xiao, R Zhou, L Zhang, X Xi, X Shi, W Wang, J Wang.<br>
</li>
<li>
	Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?. <i>arXiv preprint arXiv:2507.04391</i>.<br>
	R Zhou, M Xu, S Chen, J Liu, Y Li, X Lin, Z Chen, J He.<br>
</li>
<li>
	From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling. <i>arXiv preprint arXiv:2506.00027</i>.<br>
	Z Chen, Y Wang, T Xiao, R Zhou, X Yang, W Wang, Z Sui, J Wang.<br>
</li>
<li>
	Introducing LongCat-Flash-Thinking: A Technical Report. <i>arXiv preprint arXiv:2509.18883</i>.<br>
	MLC Team, A Gui, B Li, B Tao, B Zhou, B Chen, C Zhang, C Han, C Yang, ....<br>
</li>
<li>
	Leveraging invariant principle for heterophilic graph structure distribution shifts. <i>Proceedings of the ACM on Web Conference 2025, 1196-1204</i>.<br>
	J Yang, Z Chen, T Xiao, Y Lin, W Zhang, K Kuang.<br>
</li>
<li>
	Longcat-flash technical report. <i>arXiv preprint arXiv:2509.01322</i>.<br>
	MLC Team, B Li, B Lei, B Wang, B Rong, C Wang, C Zhang, C Gao, ....<br>
</li>
<li>
	Mix data or merge models? balancing the helpfulness, honesty, and harmlessness of large language model via model merging. <i>arXiv preprint arXiv:2502.06876</i>.<br>
	J Yang, D Jin, A Tang, L Shen, D Zhu, Z Chen, Z Zhao, D Wang, Q Cui, ....<br>
</li>
<li>
	On a connection between imitation learning and RLHF. <i>ICLR 2025</i>.<br>
	T Xiao, Y Yuan, M Li, Z Chen, VG Honavar.<br>
</li>
<li>
	Revisiting Scaling Laws for Language Models: The Role of Data Quality and Training Strategies. <i>Proceedings of the 63rd Annual Meeting of the Association for Computational …</i>.<br>
	Z Chen, S Wang, T Xiao, Y Wang, S Chen, X Cai, J He, J Wang.<br>
</li>
<li>
	SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity. <i>arXiv preprint arXiv:2503.01506</i>.<br>
	X Xi, D Kong, J Yang, J Yang, Z Chen, W Wang, J Wang, X Cai, S Zhang, ....<br>
</li>
<li>
	SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters. <i>ICLR 2025</i>.<br>
	T Xiao, Y Yuan, Z Chen, M Li, S Liang, Z Ren, VG Honavar.<br>
</li>
<li>
	Simple Denoising Diffusion Language Models. <i>arXiv preprint arXiv:2510.22926</i>.<br>
	H Zhu, Z Chen, S Zhou, Z Xie, Y Yuan, Z Guo, S Xu, H Zhang, V Honavar, ....<br>
</li>
<li>
	Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs. <i>arXiv preprint arXiv:2507.10613</i>.<br>
	Z Chen, S Wang, T Xiao, Y Wang, S Chen, X Cai, J He, J Wang.<br>
</li>
<li>
	Unifying adversarial perturbation for graph neural networks. <i>arXiv preprint arXiv:2509.00387</i>.<br>
	J Yang, R Zhang, Z Chen, F Wu, K Kuang.<br>
</li>

<h3><strong>2024</strong></h3>

<li>
	Discovering invariant neighborhood patterns for heterophilic graphs. <i>arXiv preprint arXiv:2403.10572</i>.<br>
	J Yang, R Zhang, Z Chen, T Xiao, Y Wang, F Wu, K Kuang.<br>
</li>
<li>
	DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation. <i>Advances in Neural Information Processing Systems 37, 55443-55469</i>.<br>
	Y Ai, X Zhou, H Huang, X Han, Z Chen, Q You, H Yang.<br>
</li>
<li>
	Explaining length bias in llm-based preference evaluations. <i>arXiv preprint arXiv:2407.01085</i>.<br>
	Z Hu, L Song, J Zhang, Z Xiao, T Wang, Z Chen, NJ Yuan, J Lian, K Ding, ....<br>
</li>
<li>
	Intelligent model update strategy for sequential recommendation. <i>Proceedings of the ACM Web Conference 2024, 3117-3128</i>.<br>
	Z Lv, W Zhang, Z Chen, S Zhang, K Kuang.<br>
</li>
<li>
	Learning to reweight for generalizable graph neural network. <i>Proceedings of the AAAI conference on artificial intelligence 38 (8), 8320-8328</i>.<br>
	Z Chen, T Xiao, K Kuang, Z Lv, M Zhang, J Yang, C Lu, H Yang, F Wu.<br>
</li>
<li>
	Mapo: Boosting large language model performance with model-adaptive prompt optimization. <i>EMNLP 2023 Findings of the Association for Computational Linguistics</i>.<br>
	Y Chen, Z Wen, G Fan, Z Chen, W Wu, D Liu, Z Li, B Liu, Y Xiao.<br>
</li>
<li>
	Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace. <i>ICLR 2025</i>.<br>
	J Yang, A Tang, D Zhu, Z Chen, L Shen, F Wu.<br>
</li>
<li>
	Open-finllms: Open multimodal large language models for financial applications. <i>arXiv preprint arXiv:2408.11878</i>.<br>
	J Huang, M Xiao, D Li, Z Jiang, Y Yang, Y Zhang, L Qian, Y Wang, X Peng, ....<br>
</li>
<li>
	Pareto graph self-supervised learning. <i>ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and …</i>.<br>
	Z Chen, T Xiao, D Wang, M Zhang.<br>
</li>
<li>
	Rethinking llm-based preference evaluation. <i>arXiv e-prints, arXiv: 2407.01085</i>.<br>
	Z Hu, L Song, J Zhang, Z Xiao, J Wang, Z Chen, J Zhao, H Xiong.<br>
</li>
<li>
	Robust Heterophily Graph Learning via Uniformity Augmentation. <i>Proceedings of the 33rd ACM International Conference on Information and …</i>.<br>
	X Yang, Z Chen, Y Zou.<br>
</li>
<li>
	Scaling laws across model architectures: A comparative analysis of dense and MoE models in large language models. <i>EMNLP 2024 Proceedings of the 2024 Conference on Empirical Methods in …</i>.<br>
	S Wang, Z Chen, B Li, K He, M Zhang, J Wang.<br>
</li>
<li>
	Transferring causal mechanism over meta-representations for target-unknown cross-domain recommendation. <i>ACM Transactions on Information Systems 42 (4), 1-27</i>.<br>
	S Zhang, Q Miao, P Nie, M Li, Z Chen, F Feng, K Kuang, F Wu.<br>
</li>

<h3><strong>2023</strong></h3>

<li>
	Duet: A tuning-free device-cloud collaborative parameters generation framework for efficient device model generalization. <i>Proceedings of the ACM Web Conference 2023, 3077-3085</i>.<br>
	Z Lv, W Zhang, S Zhang, K Kuang, F Wang, Y Wang, Z Chen, T Shen, ....<br>
</li>
<li>
	Invariant graph neural network for out-of-distribution nodes. <i>Proceedings of the 2023 15th International Conference on Machine Learning …</i>.<br>
	Z Chen, Y Gong, L Yang, J Zhang, W Zhang, S He, X Zhang.<br>
</li>
<li>
	Map: Towards balanced generalization of iid and ood through model-agnostic adapters. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision …</i>.<br>
	M Zhang, J Yuan, Y He, W Li, Z Chen, K Kuang.<br>
</li>
<li>
	Multi-level correlation network for few-shot image classification. <i>2023 IEEE International Conference on Multimedia and Expo (ICME), 2909-2914</i>.<br>
	Y Dang, M Sun, M Zhang, Z Chen, X Zhang, Z Wang, D Wang.<br>
</li>
<li>
	Reconsidering learning objectives in unbiased recommendation: A distribution shift perspective. <i>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and …</i>.<br>
	T Xiao, Z Chen, S Wang.<br>
</li>
<li>
	Robustness Evaluation of Multi-Agent Reinforcement Learning Algorithms using GNAs. <i>ICLR 2023 International Conference on Learning Representations</i>.<br>
	X Zhang, W Zhang, Y Gong, L Yang, J Zhang, Z Chen, S He.<br>
</li>
<li>
	Simple and asymmetric graph contrastive learning without augmentations. <i>Advances in neural information processing systems 36, 16129-16152</i>.<br>
	T Xiao, H Zhu, Z Chen, S Wang.<br>
</li>
<li>
	Using speech emotion recognition as a longitudinal biomarker for Alzheimer’s disease. <i>Int. J. Biomed. Biol. Eng. 17 (11), 267-272</i>.<br>
	Y Gong, L Yang, J Zhang, Z Chen, S He, X Zhang, W Zhang.<br>
</li>

<h3><strong>2022</strong></h3>

<li>
	A novel detachable gate driver unit with ultralow inductance for integrated gate commutated thyristor. <i>IEEE Transactions on Power Electronics 37 (12), 14000-14004</i>.<br>
	J Shang, Z Chen, B Zhao, Z Yu, J Wu, L Dong, R Zeng.<br>
</li>
<li>
	Ba-gnn: On learning bias-aware graph neural network. <i>2022 IEEE 38th International Conference on Data Engineering (ICDE), 3012-3024</i>.<br>
	Z Chen, T Xiao, K Kuang.<br>
</li>
<li>
	Decoupled self-supervised learning for graphs. <i>Advances in Neural Information Processing Systems 35, 620-634</i>.<br>
	T Xiao, Z Chen, Z Guo, Z Zhuang, S Wang.<br>
</li>
<li>
	End-to-end open-set semi-supervised node classification with out-of-distribution detection. <i>IJCAI</i>.<br>
	T Huang, D Wang, Y Fang, Z Chen.<br>
</li>
<li>
	Knowledge distillation of transformer-based language models revisited. <i>arXiv preprint arXiv:2206.14366</i>.<br>
	C Lu, J Zhang, Y Chu, Z Chen, J Zhou, F Wu, H Chen, H Yang.<br>
</li>
<li>
	Learn goal-conditioned policy with intrinsic motivation for deep reinforcement learning. <i>Proceedings of the AAAI conference on artificial intelligence 36 (7), 7558-7566</i>.<br>
	J Liu, D Wang, Q Tian, Z Chen.<br>
</li>
<li>
	MetaNetwork: A Task-agnostic Network Parameters Generation Framework for Improving Device Model Generalization.. <i>CoRR</i>.<br>
	Z Lv, F Wang, K Kuang, Y Wang, Z Chen, T Shen, H Yang, F Wu.<br>
</li>
<li>
	Reconsidering Learning Objectives in Unbiased Recommendation with Unobserved Confounders. <i>arXiv preprint arXiv:2206.03851</i>.<br>
	T Xiao, Z Chen, S Wang.<br>
</li>
<li>
	Representation matters when learning from biased feedback in recommendation. <i>Proceedings of the 31st ACM International Conference on Information …</i>.<br>
	T Xiao, Z Chen, S Wang.<br>
</li>
<li>
	The role of deconfounding in meta-learning. <i>International Conference on Machine Learning, 10161-10176</i>.<br>
	Y Jiang, Z Chen, K Kuang, L Yuan, X Ye, Z Wang, F Wu, Y Wei.<br>
</li>
<li>
	Towards Bridging Algorithm and Theory for Unbiased Recommendation. <i>arXiv preprint arXiv:2206.03851</i>.<br>
	T Xiao, Z Chen, S Wang.<br>
</li>

<h3><strong>2021</strong></h3>

<li>
	Deep transfer tensor decomposition with orthogonal constraint for recommender systems. <i>Proceedings of the AAAI conference on artificial intelligence 35 (5), 4010-4018</i>.<br>
	Z Chen, Z Xu, D Wang.<br>
</li>
<li>
	Learning how to propagate messages in graph neural networks. <i>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data …</i>.<br>
	T Xiao, Z Chen, D Wang, S Wang.<br>
</li>
<li>
	Multi-modal meta continual learning. <i>2021 International Joint Conference on Neural Networks (IJCNN), 1-8</i>.<br>
	S Gai, Z Chen, D Wang.<br>
</li>
<li>
	Pareto self-supervised training for few-shot learning. <i>Proceedings of the IEEE/CVF conference on computer vision and pattern …</i>.<br>
	Z Chen, J Ge, H Zhan, S Huang, D Wang.<br>
</li>

<h3><strong>2020</strong></h3>

<li>
	A Hybrid aspect based latent factor model for recommendation. <i>Chinese Journal of Electronics 29 (3), 482-490</i>.<br>
	H Yuan, Z Chen, J Yang, S Wang, J Geng, C Ke.<br>
</li>
<li>
	Global and local tensor factorization for multi-criteria recommender system. <i>Patterns 1 (2)</i>.<br>
	S Wang, J Yang, Z Chen, H Yuan, J Geng, Z Hai.<br>
</li>

</ol>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1910977-2");
pageTracker._initData();
pageTracker._trackPageview();
</script>

<h2><a id=News>Talks</a></h2>
<ul>
<li>[Nov. 2023]		CCIR 2023 talk: Directions of Recommendation in the Era of LLM (<FONT style="FONT-FAMILY: 楷体">推荐系统大模型的三条技术路径</FONT>) [<A HREF="publications/202311-Ruobing_Xie.pdf" target=_blank>slides</A>].
</ul>

<h2><a name=Education>Education and Experience</a></h2>
<ul>
<li>Jul. 2017 - present. Senior Researcher, Tencent, Beijing.
<li>Sep. 2014 - Jul. 2017. M.S. candidate, Dept. of Computer Science and Technology, Tsinghua University, Beijing, China.
<li>Sep. 2010 - Jul. 2014. B.S., Dept. of Computer Science and Technology, Tsinghua University, Beijing, China.
<li>Sep. 2007 - Jul. 2010. Yali high school, Changsha, Hunan, China.
</ul>

<h2><a name=Awards>Awards</a></h2>
<ul>
<li>2024. Best Full Paper Award of RecSys 2024.
<li>2024. 2024 World's Top 2 % Scientists.
<li>2024. Best Paper Candidate of CCIR 2024.
<li>2023. Best Short Paper Award of RecSys 2023.
<li>2023. Paper Award Nominations of TheWebConf 2023 (spotlight).
<li>2022. AI 2000 Most Influential Scholar Award Honorable Mention (2022,2023,2024).
<li>2020. The first prize of the Natural Science Award (Science and Technology) of the Ministry of Education.
<li>2017. Beijing Excellent Graduate; Excellent Graduate & Master Thesis, Tsinghua University. ( Top 1.3%, 71 in 5304 )
<li>2016. Siebel Scholars. [<A HREF="http://www.siebelscholars.com" target=_blank>link</A>]
<li>2014. Excellent Graduate, Tsinghua University.
</ul>

<h2><a name=Academic>Academic Services</a></h2>
<ul>
<li>Scholar of Young Talent Promoting Project of CAST.
<li>Member of Chinese Information Processing Society of China (CIPS), Youth Working Committee [<A HREF="http://www.cipsc.org.cn/qngw/" target=_blank>link</A>]
<li>Member of Chinese Information Processing Society of China (CIPS), Social Media Processing Committee & Information Retrieval Committee
<li>AC/SPC: ACL, EMNLP, NAACL, IJCAI, AAAI
<li>PC: NeurIPS, ICML, ICLR, KDD, WWW, SIGIR, CVPR, ICCV, AAAI, IJCAI, ACMMM, EMNLP, NAACL, COLING, CIKM, WSDM, RecSys, ECML/PKDD, SDM, ICME, NLPCC, AACL, COLM
<li>Journal reviewer: CL, TOIS, TKDE, SCIS, IPM, JWS, Information Sciences, FCS, Neurocomputing, TBD, MIR
</ul>

<hr>
</body>
</html>